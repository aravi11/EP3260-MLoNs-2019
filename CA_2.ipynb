{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data prepration\n",
    "    \n",
    "## 1.1 Load data and preprocessing\n",
    "In this part we import libraries and data set. Data set has 9 columns and 2075259 samples. It also has missing values. The first step is to load data set and handle missing values. Also the first two columns contain time and date, which we eliminate in our first version of code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahab/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of data set is: (2075259, 9)\n",
      "The first three rows of data set:\n",
      "          Date      Time Global_active_power Global_reactive_power  Voltage  \\\n",
      "0  16/12/2006  17:24:00               4.216                 0.418  234.840   \n",
      "1  16/12/2006  17:25:00               5.360                 0.436  233.630   \n",
      "2  16/12/2006  17:26:00               5.374                 0.498  233.290   \n",
      "\n",
      "  Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
      "0           18.400          0.000          1.000            17.0  \n",
      "1           23.000          0.000          1.000            16.0  \n",
      "2           23.000          0.000          2.000            17.0  \n",
      "The last three rows of data set:\n",
      "                Date      Time Global_active_power Global_reactive_power  \\\n",
      "2075256  26/11/2010  21:00:00               0.938                     0   \n",
      "2075257  26/11/2010  21:01:00               0.934                     0   \n",
      "2075258  26/11/2010  21:02:00               0.932                     0   \n",
      "\n",
      "        Voltage Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
      "2075256  239.82              3.8              0              0             0.0  \n",
      "2075257   239.7              3.8              0              0             0.0  \n",
      "2075258  239.55              3.8              0              0             0.0  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"household_power_consumption.txt\",\";\")\n",
    "\n",
    "print(\"The size of data set is:\",data.shape)\n",
    "print(\"The first three rows of data set:\\n\",data.head(3))\n",
    "print(\"The last three rows of data set:\\n\",data.tail(3))\n",
    "\n",
    "X = pd.DataFrame(data.iloc[:,2:6], columns=[\"Global_active_power\",\"Global_reactive_power\",\"Voltage\",\"Global_intensity\"])\n",
    "Y = pd.DataFrame(data.iloc[:,7], columns=[\"Sub_metering_2\"])\n",
    "\n",
    "X = X.replace({'?':0})\n",
    "Y = Y.replace({'?':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 train and test sets\n",
    "Befor starting the training we need to split our data to train and test. In traditional learning algorithms, people usually use 70%/30% train/test or 60%/20%/20% train/dev/test set. Nowadays, because of having large data sets, people usually use 98%/1%/1% train/dev/test set. In this assignment we used the traditional way of spliting data into train and test with the ratio of 70%/30% of train/test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set (X):  (1452681, 4)\n",
      "Size of train set (Y):  (1452681, 1)\n",
      "Size of test set (X):  (622578, 4)\n",
      "Size of test set (Y):  (622578, 1)\n"
     ]
    }
   ],
   "source": [
    "X_features = X.columns\n",
    "Y_features = Y.columns\n",
    "XY = pd.concat([X[X_features], Y[Y_features]], axis=1)\n",
    "\n",
    "# Split XY into training set and test set of equal size\n",
    "train, test = train_test_split(XY, test_size = 0.30)\n",
    "# Sort the train and test sets after index (which became unsorted through sampling)\n",
    "train = train.sort_index(axis=0)\n",
    "test = test.sort_index(axis=0)\n",
    "\n",
    "# Extract X,Y components from test and train sets\n",
    "X_train = train[X_features].astype(float); X_test = test[X_features].astype(float)\n",
    "Y_train = train[Y_features].astype(float); Y_test = test[Y_features].astype(float)\n",
    "\n",
    "print(\"Size of train set (X): \",X_train.shape)\n",
    "print(\"Size of train set (Y): \",Y_train.shape)\n",
    "print(\"Size of test set (X): \",X_test.shape)\n",
    "print(\"Size of test set (Y): \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic ridge regression with different optimizers\n",
    "\n",
    "The goal of this part is to implement logistic ridge regression with four different optimizers (GD, SGD, SAG, SVRG). We define logistic ridge as following:  \n",
    "\n",
    "\\begin{equation*}\n",
    "f(w) = \\frac{1}{N} \\sum_{i\\in[N]} f_i(w) + \\lambda ||w||_2^2\n",
    "\\end{equation*}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation*}\n",
    "f_i(w) = \\log(1+\\exp(y_iw^Tx_i))\n",
    "\\end{equation*}\n",
    "\n",
    "In the following section we each of optimizers and will try to optimize the specified goal function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_gradient_GD(X, Y, w, lambda_):\n",
    "    Z = np.matmul(X,np.diagflat(Y))\n",
    "    D,N = Z.shape\n",
    "    Z_ = -1 * np.matmul(w.T , Z)\n",
    "    A = 1/(1+np.exp(Z_))\n",
    "    A_ = np.diagflat(A-1)\n",
    "    G = 1 / N * np.sum(np.matmul(Z , A_), axis=1,keepdims=True) + 2 * lambda_ * w\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.76964991]\n",
      " [-0.68160782]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[2,3,3],[1,4,3]])\n",
    "Y = np.array([[1],[2],[5]])\n",
    "w = np.array([[0.01],[0.21]])\n",
    "\n",
    "print(function_gradient_GD(X,Y,w,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def solver(x,y, w, alpha = 0.1, num_iters = 1000, lambda_ = 0.1, epsilon = 0.0001, optimizer = \"GD\"):\n",
    "    if (optimizer == \"GD\") :\n",
    "        for i in range(num_iters):\n",
    "            g = function_gradient_GD(x, y, w, lambda_)\n",
    "            w = w - alpha * g\n",
    "            if (np.linalg.norm(g) <= epsilon):\n",
    "                break\n",
    "    elif (optimizer == \"SGD\"):\n",
    "        for i in range(num_iters):\n",
    "            D,N = x.shape\n",
    "            sample_no = int(np.random.random(1) * N)\n",
    "            Z = np.matmul(x,np.diagflat(y))\n",
    "            A = -1 * np.matmul(w.T, Z[:,i])\n",
    "            G = (Z[:,[i]]*float(1/(1+np.exp(A))-1))+ 2 * lambda_ * w\n",
    "            w = w - alpha * G\n",
    "    elif (optimizer == \"SVRG\"):\n",
    "        T = 100\n",
    "        K = math.floor(num_iters/T)\n",
    "        Z = np.matmul(x,np.diagflat(y))\n",
    "        N = x.shape[1]\n",
    "        for k in range(K):\n",
    "            wz = np.matmul(w.T , Z)\n",
    "            diag = np.diagflat(1/(1+np.exp(-1*wz))-np.ones((1,N)))\n",
    "            Ga_ = np.matmul(Z , diag)\n",
    "            ga_ = (1/N) * np.matmul(Ga_ , np.ones((N,1)))\n",
    "            for t in range(T):\n",
    "                r = int(np.random.random(1) * N)\n",
    "                col = Z[:,[r]]\n",
    "                #col = col.reshape((col.shape[0],1))\n",
    "                g = np.matmul(col , (1/(1+np.exp(-1 * np.matmul(w.T , col)))-1))\n",
    "                Ga_col = Ga_[:,r]\n",
    "                Ga_col = Ga_col.reshape(Ga_col.shape[0],1)\n",
    "                w = w - alpha * (g - Ga_col + ga_ + 2 * lambda_ * w)\n",
    "            \n",
    "    elif (optimizer == \"SAG\"):\n",
    "        Z = np.matmul(x,np.diagflat(y))\n",
    "        d = x.shape[0]\n",
    "        N = x.shape[1]\n",
    "        G = np.zeros((d,N))\n",
    "\n",
    "        for k in range(num_iters):\n",
    "            r = int(np.random.random(1) * N)\n",
    "            col = Z[:,r].reshape(Z[:,r].shape[0],1)\n",
    "            B = np.matmul(col ,(1/(1 + np.exp(-1 * np.matmul(w.T,col))) - 1))\n",
    "            B = B.reshape(B.shape[0])\n",
    "            G[:,r] = B\n",
    "            g = (1/N) * np.matmul(G , np.ones((N,1)))  + 2 * lambda_ * w\n",
    "            w = w - alpha * g\n",
    "            \n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(x,y,w,lambda_ = 0.01):\n",
    "    D, N = x.shape\n",
    "    value = 0\n",
    "    for i in range(N):\n",
    "        Z = -1 * y[i] * np.matmul(w.T , (x[:,i]).reshape(D,1))\n",
    "        value += np.log(1+np.exp(Z))\n",
    "    norm = np.linalg.norm(w)\n",
    "    c = lambda_ * norm ** 2\n",
    "    return value/N + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25389454]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[2,3,5],[1,4,4]])\n",
    "Y = np.array([[1],[2],[7]])\n",
    "w = np.array([[0.01],[0.21]])\n",
    "\n",
    "print(cost(X,Y,w,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of GD after convergence: \n",
      " [[  1.39374748e-03]\n",
      " [  1.24737032e-04]\n",
      " [  1.30549938e-01]\n",
      " [  4.42132906e-03]]\n",
      "Cost of GD after convergence:  [[ 0.47035549]]\n",
      "Training time for GD:  24.653496980667114\n",
      "Weights of SGD after convergence: \n",
      " [[  3.99765007e-04]\n",
      " [  1.23881580e-05]\n",
      " [  3.82345319e-02]\n",
      " [  1.69091126e-03]]\n",
      "Cost of SGD after convergence:  [[ 0.47021625]]\n",
      "Training time for SGD:  61.7726354598999\n",
      "Weights of SVRG after convergence: \n",
      " [[  2.59821442e-04]\n",
      " [  2.56473699e-05]\n",
      " [  3.57130200e-02]\n",
      " [  1.10374980e-03]]\n",
      "Cost of SVRG after convergence:  [[ 0.47022849]]\n",
      "Training time for SVRG:  1.4326896667480469\n",
      "Weights of SAG after convergence: \n",
      " [[  1.02091694e-03]\n",
      " [  5.44494501e-05]\n",
      " [  5.61698936e-02]\n",
      " [  2.77872666e-03]]\n",
      "Cost of SAG after convergence:  [[ 0.4702167]]\n",
      "Training time for SAG:  0.1322011947631836\n"
     ]
    }
   ],
   "source": [
    "y = np.array(Y_train.iloc[0:6000])\n",
    "x = np.array(X_train.iloc[0:6000,:])\n",
    "\n",
    "N,D = x.shape\n",
    "w = np.random.rand(D,1)*0.01\n",
    "\n",
    "##################GD###################\n",
    "start = time.time()\n",
    "gde = solver(x.T,y,w,num_iters=100)\n",
    "end = time.time()\n",
    "print(\"Weights of GD after convergence: \\n\",gde)\n",
    "\n",
    "err = cost(x.T,y,gde)\n",
    "print(\"Cost of GD after convergence: \",err)\n",
    "\n",
    "print(\"Training time for GD: \", end-start)\n",
    "\n",
    "##################SGD###################\n",
    "start = time.time()\n",
    "gde = solver(x.T,y,w, num_iters=500,optimizer = \"SGD\")\n",
    "end = time.time()\n",
    "print(\"Weights of SGD after convergence: \\n\",gde)\n",
    "\n",
    "err = cost(x.T,y,gde)\n",
    "print(\"Cost of SGD after convergence: \",err)\n",
    "print(\"Training time for SGD: \", end-start)\n",
    "##################SVRG###################\n",
    "start = time.time()\n",
    "gde = solver(x.T,y,w, num_iters=1000,optimizer = \"SVRG\")\n",
    "end = time.time()\n",
    "print(\"Weights of SVRG after convergence: \\n\",gde)\n",
    "\n",
    "err = cost(x.T,y,gde)\n",
    "print(\"Cost of SVRG after convergence: \",err)\n",
    "print(\"Training time for SVRG: \", end-start)\n",
    "##################SAG###################\n",
    "start = time.time()\n",
    "gde = solver(x.T,y,w, num_iters=100,optimizer = \"SAG\")\n",
    "end = time.time()\n",
    "print(\"Weights of SAG after convergence: \\n\",gde)\n",
    "\n",
    "err = cost(x.T,y,gde)\n",
    "print(\"Cost of SAG after convergence: \",err)\n",
    "print(\"Training time for SAG: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ti= np.zeros((100,4))\n",
    "cost_= np.zeros((100,4))\n",
    "for i in range(100):\n",
    "    ##################GD###################\n",
    "    start = time.time()\n",
    "    gde = solver(x.T,y,w,num_iters=i)\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,0] = cost(x.T,y,gde)\n",
    "\n",
    "    ti[i,0] = end-start\n",
    "\n",
    "    ##################SGD###################\n",
    "    start = time.time()\n",
    "    gde = solver(x.T,y,w, num_iters=i,optimizer = \"SGD\")\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,1] = cost(x.T,y,gde)\n",
    "    \n",
    "    ti[i,1] = end-start\n",
    "    ##################SVRG###################\n",
    "    start = time.time()\n",
    "    gde = solver(x.T,y,w, num_iters=i,optimizer = \"SVRG\")\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,2] = cost(x.T,y,gde)\n",
    "    \n",
    "    ti[i,2] = end-start\n",
    "    ##################SAG###################\n",
    "    start = time.time()\n",
    "    gde = solver(x.T,y,w, num_iters=i,optimizer = \"SAG\")\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,3] = cost(x.T,y,gde)\n",
    "    ti[i,3] = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results\n",
    "\n",
    "As we can compare we can see that the GD is better than other algorithms in terms of accuracy but it is slower than other algorithms. GD even with just 100 of iteration take a long time to be trained (24 seconds in comparison with 10 seconds for SGD for example) and interestingly it converges just with these few numbers of iterations. Results shows that SAG is really good in comparision with others. This algorithm is as accurate as others and also it is really fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tunning the lambda paramter\n",
    "\n",
    "To tune hyper-paramter there are different methods. One methos is to define space for hyper-parameters of the model, for example here we have a space with just one dimension. Then we can define points in this space and train our model with the paramters of each point. The following image shows how we can define and search space to tune hyper-paramters. Here we consider lambda as a hyper-paramter of the model and then with different values of lambda we compute cost (error metric, it can be MSE, NMAE, or any other metric here for simplicity we used cost which was defined before) for each model. \n",
    "\n",
    "![title](search.png)\n",
    "\n",
    "As the figure shows we can search the space with two different approaches.\n",
    "\n",
    "- Grid search\n",
    "- Random search\n",
    "\n",
    "Both of these methods are very common but the Random seach in first round is better. Because in this search it is mostly probable to find a good solution. We can also use our knowledge to define the space, for example we can define a logarithmic scale for our space. We tune this hyper-paramter just for the logistic ridge regression with SAG optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for la in space:\n",
    "    start = time.time()\n",
    "    gde = solver(x.T,y,w, num_iters=100,optimizer = \"SAG\", lambda_=la)\n",
    "    end = time.time()\n",
    "    print(\"Weights of SAG after convergence: \\n\",gde)\n",
    "\n",
    "    err = cost(x.T,y,gde)\n",
    "    print(\"Cost of SAG after convergence: \",err)\n",
    "    print(\"Training time for SAG: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
